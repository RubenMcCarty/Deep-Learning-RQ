{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubenMcCarty/Deep-Learning-RQ/blob/main/DATA_SCIENTIST_EXAMEN_BIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwMjCGXvdwM6"
      },
      "source": [
        "### **1. Solicitud de tarjetas de credito en el banco BIC**\n",
        "### [Profesor: M.Sc.Ruben Quispe](https://www.linkedin.com/in/msc-rub%C3%A9n-quispe-l/)\n",
        "\n",
        "\n",
        "Caso: El banco BIC  recibe muchas solicitudes de tarjetas de crédito. Muchas de ellas son rechazadas por diversas razones, como deudas elevadas de préstamos, bajos niveles de ingresos o demasiadas consultas sobre el informe crediticio de una persona, etc. El análisis manual de estas aplicaciones es pesado, propenso a errores y requiere mucho tiempo. Afortunadamente, esta tarea se puede automatizar con el poder del aprendizaje automático y prácticamente todos los bancos comerciales lo hacen hoy en día. Tu tarea en este examen es desarrollar un modelo predictivo de aprobación de tarjetas de crédito utilizando técnicas de aprendizaje automático, tal como lo hacen los bancos reales.\n",
        "\n",
        "Usarás un dataset que tiene la información necesaria para completar la tarea propuesta, el cual te hemos mandado por correo o también lo puedes descargar de [aquí](https://drive.google.com/file/d/1PGysvgrvHx3Z3lFr6jc-oSN9l7l6Z71E/view?usp=sharing). Debes cargarlo al entorno de ejecución para poder usarlo más adelante. \n",
        "\n",
        "La estructura de este notebook es la siguiente:\n",
        "\n",
        "- Primero, comienza cargando y viendo el dataset.\n",
        "- Verás  que el dataset tiene una mezcla de variables numéricas y no numéricas, que contiene valores de diferentes rangos, además de que contiene algunos datos faltantes.\n",
        "- Tendrás que preprocesar el dataset para garantizar que el modelo de ML pueda hacer buenas predicciones.\n",
        "- Una vez que los datos estén en buen estado, harás un análisis exploratorio de datos para derivar intuiciones.\n",
        "- Finalmente, construirás un modelo de que pueda predecir si se aceptará o no la solicitud de una tarjeta de crédito de una persona.\n",
        "\n",
        "**Nota:** Los nombres de las columnas son anónimos sin embargo esto no debe impedir realizar una correcta evaluación.\n",
        "\n",
        "**Nota:** Cada vez que veas ... o # ... TU CÓDIGO AQUÍ ... debes completar con una o más líneas de código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Lp4UNnZdqKr"
      },
      "outputs": [],
      "source": [
        "# Importa pandas\n",
        "# ... TU CÓDIGO AQUÍ ...\n",
        "\n",
        "# Carga el dataset usando read_csv\n",
        "cc_apps = ...\n",
        "\n",
        "# Inspecciona los datos (haz print)\n",
        "# ... TU CÓDIGO AQUÍ ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eukbHejzis3S"
      },
      "source": [
        "### **2. Inspecciona las solicitudes**\n",
        "\n",
        "El resultado puede parecer un poco confuso a primera vista, pero intentarás descubrir cuáles son las variables más importantes. Como puedes ver desde el primer vistazo a los datos, el dataset tiene una mezcla de características numéricas y no numéricas. Esto se puede solucionar con un preprocesamiento, pero antes de hacerlo, revisa un poco más el dataset para ver si hay otros problemas que deben solucionarse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u2bu9W6jdeE"
      },
      "outputs": [],
      "source": [
        "# Haz print del resumen estadístico\n",
        "cc_apps_description = ...\n",
        "print(cc_apps_description)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Haz print de la información del dataframe\n",
        "cc_apps_info = ...\n",
        "print(cc_apps_info)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Inspecciona las últimas 17 filas en búsqueda de valores faltantes en el dataset\n",
        "# ... TU CÓDIGO AQUÍ ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNh1eFEgmQQp"
      },
      "source": [
        "### **3. Divide el conjunto de datos en train y test**\n",
        "\n",
        "Ahora, divide los datos en un conjunto de train y uno de test para prepararlos para dos fases diferentes del modelado: entrenamiento y prueba. Idealmente, no se debe usar información de los datos de prueba para preprocesar los datos de entrenamiento o para dirigir el proceso de entrenamiento de un modelo de aprendizaje automático. Por lo tanto, primero dividirás los datos y luego los preprocesarás.\n",
        "\n",
        "Además, por indicación del banco debes eliminar las variables en la posición 11 y 13."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox_6VnL0mPLv"
      },
      "outputs": [],
      "source": [
        "# Importa train_test_split\n",
        "# ... TU CÓDIGO AQUÍ ...\n",
        "\n",
        "# Elimina las variables 11 y 13\n",
        "cc_apps = ...\n",
        "\n",
        "# Divide el conjunto en sets de train y test\n",
        "cc_apps_train, cc_apps_test = train_test_split(..., test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V2fITMcnqTc"
      },
      "source": [
        "### **4. Manejo de los valores faltantes (parte 1)**\n",
        "\n",
        "Ahora que haz dividido los datos, puedes manejar algunos de los problemas que identificaste al inspeccionar el DataFrame, que incluyen:\n",
        "\n",
        "- El dataset contiene datos numéricos y no numéricos (específicamente datos que son de tipo float64, int64 y object). Específicamente, las variables 2, 7, 10 y 14 contienen valores numéricos (de tipo float64, float64, int64 e int64 respectivamente) y todas las demás variables contienen valores no numéricos.\n",
        "- El dataset también contiene valores de varios rangos. Algunas variables tienen un rango de valores de 0 a 28, algunas de 2 a 67 y otras de 1017 a 100000. Además de esto, pudiste obtener información estadística útil (como media, máx. y mín.) sobre las variables que tienen valores numéricos.\n",
        "- Finalmente, el dataset tiene valores faltantes, de los cuales te ocuparás en esta tarea. Los valores que faltan en el dataset están etiquetados con '?', que puedes ver en el resultado de la última celda de la segunda tarea.\n",
        "\n",
        "Ahora, reemplaza temporalmente estos signos de interrogación de valor faltante con NaN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG6VDvysofhc"
      },
      "outputs": [],
      "source": [
        "# Importa numpy\n",
        "# ... TU CÓDIGO AQUÍ ...\n",
        "\n",
        "# Reemplaza los '?' con NaN en los sets de train y test\n",
        "# ... TU CÓDIGO AQUÍ ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9063zEauojlv"
      },
      "source": [
        "### **5. Manejo de los valores faltantes (parte 2)**\n",
        "\n",
        "Reemplazaste todos los signos de interrogación con NaN. Esto te ayudará en el próximo tratamiento de valores faltantes que vas a realizar.\n",
        "\n",
        "Una pregunta importante que surge aquí es ¿por qué le damos tanta importancia a los valores faltantes? ¿No pueden simplemente ser ignorados? Ignorar los valores faltantes puede afectar en gran medida el rendimiento de un modelo de aprendizaje automático. Al ignorar los valores faltantes, el modelo de aprendizaje automático puede perder información sobre el dataset que puede ser útil para su entrenamiento. Entonces, hay muchos modelos que no pueden manejar los valores faltantes implícitamente, como el análisis discriminante lineal (LDA).\n",
        "\n",
        "Para evitar este problema, vas a imputar los valores faltantes con la estrategia imputación media."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o91Ae2KpFES"
      },
      "outputs": [],
      "source": [
        "# Haz print al recuerto de NaNs en el dataset para verificar\n",
        "# ... TU CÓDIGO AQUÍ ...\n",
        "\n",
        "# Imputa los valores faltantes con la media\n",
        "# ... TU CÓDIGO AQUÍ ...\n",
        "\n",
        "# Haz print al recuerto de NaNs en el dataset para verificar\n",
        "# ... TU CÓDIGO AQUÍ ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYMENdRIpY8S"
      },
      "source": [
        "### **6. Manejo de los valores faltantes (parte 3)**\n",
        "\n",
        "Todavía faltan algunos valores por imputar para las columnas 0, 1, 3, 4, 5, 6 y 13. Todas estas columnas contienen datos no numéricos y es por eso que la estrategia de imputación media no funcionaría aquí. Esto necesita un tratamiento diferente.\n",
        "\n",
        "Imputa estos valores faltantes con los valores más frecuentes presentes en las respectivas columnas. Esta es una buena práctica cuando se trata de imputar valores faltantes para datos categóricos en general."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaZi94CTKhSd"
      },
      "outputs": [],
      "source": [
        "# Haz print al recuerto de NaNs en el dataset para verificar\n",
        "# ... TU CÓDIGO AQUÍ ...\n",
        "\n",
        "# Itera por cada columna de cc_apps_train\n",
        "for col in ...:\n",
        "    # Valida si la columna es de tipo 'object'\n",
        "    if ... == 'object':\n",
        "        # Imputa los NaN con el valor más frecuente\n",
        "        # ... TU CÓDIGO AQUÍ ...\n",
        "\n",
        "# Haz print al recuerto de NaNs en el dataset para verificar\n",
        "# ... TU CÓDIGO AQUÍ ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU4pBKf9LRjg"
      },
      "source": [
        "### **7. Preprocesamiento de los datos (parte 1)**\n",
        "\n",
        "Los valores faltantes ahora se manejan correctamente.\n",
        "\n",
        "Todavía son necesarios unos pasos de preprocesamiento de datos menores pero esenciales antes de proceder a construir el modelo. Dividimos estos pasos de preprocesamiento restantes en dos tareas principales:\n",
        "\n",
        "1. Convierte los datos no numéricos en numéricos.\n",
        "2. Escala los valores de las variables a un rango uniforme.\n",
        "\n",
        "Primero, convierte todos los valores no numéricos en valores numéricos. Esta es buena opción porque no solo da como resultado un cálculo más rápido, sino que también muchos modelos de aprendizaje automático (como XGBoost) (y especialmente los desarrollados con scikit-learn) requieren que los datos estén en un formato estrictamente numérico. Haz esto usando el método get_dummies() de pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ul6K3sP6OVGq"
      },
      "outputs": [],
      "source": [
        "# Convierte las variables categóricas de los sets train y test independientemente\n",
        "cc_apps_train = ...\n",
        "cc_apps_test = ...\n",
        "\n",
        "# Vuelve a indexar las columnas del set test alineadas al set train (como resultado ambos sets deben tener exactamente las mismas columnas)\n",
        "# ... TU CÓDIGO AQUÍ ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DicTMiw6O8sm"
      },
      "source": [
        "### **8. Preprocesamiento de los datos (parte 2)**\n",
        "\n",
        "Solo queda un último paso de preprocesamiento de escalado antes de proceder a entrenar un modelo.\n",
        "\n",
        "Usando score crediticio como ejemplo, el score crediticio de una persona es su solvencia basada en su historial crediticio. Cuanto mayor sea este número, más confiable financieramente se considera que es una persona. Por lo tanto, un score crediticio de 1 es el más alto, ya que estamos ajustando la escala de todos los valores al rango de 0-1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzNhGaQMTUEa"
      },
      "outputs": [],
      "source": [
        "# Importa MinMaxScaler\n",
        "# ... TU CÓDIGO AQUÍ ...\n",
        "\n",
        "# Separa las variables y el target en variables separadas\n",
        "X_train, y_train = ...\n",
        "X_test, y_test = ...\n",
        "\n",
        "# Instancia MinMaxScaler y úsalo para re-escalar X_train y X_test\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "rescaledX_train = ...\n",
        "rescaledX_test = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BspFxKKUE-1"
      },
      "source": [
        "### **9. Entrenamiento de un modelo de regresión logística con el set de entrenamiento**\n",
        "\n",
        "Predecir si una solicitud de tarjeta de crédito será aprobada o no es una tarea de clasificación. El dataset contiene más instancias que corresponden al estado \"Denegado\" que instancias correspondientes al estado \"Aprobado\". Específicamente, de 690 instancias, hay 383 (55,5 %) solicitudes que fueron denegadas y 307 (44,5 %) que fueron aprobadas.\n",
        "\n",
        "Esto es un punto de referencia. Un buen modelo de aprendizaje automático debería poder predecir con precisión el estado de las aplicaciones con respecto a estas estadísticas.\n",
        "\n",
        "¿Qué modelo elegir? Una pregunta que debe hacerse es: ¿las características que afectan el proceso de decisión de aprobación de tarjetas de crédito están correlacionadas entre sí? Aunque se puede medir la correlación, eso está fuera del alcance del examen, por lo que asumirá que están correlacionados por ahora. Debido a esta correlación, debes aprovechar el hecho de que los modelos lineales generalizados funcionan bien en estos casos. Haz el modelado de aprendizaje automático con un modelo de regresión logística (un modelo lineal generalizado)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X85X3c6NWJhj"
      },
      "outputs": [],
      "source": [
        "# Importa LogisticRegression\n",
        "# ... TU CÓDIGO AQUÍ ...\n",
        "\n",
        "# Instancia un clasificador de LogisticRegression con los valores de parámetros por defecto\n",
        "logreg = ...\n",
        "\n",
        "# Entrena logreg con los sets de entrenamiento\n",
        "# ... TU CÓDIGO AQUÍ ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsmkphP9WsTB"
      },
      "source": [
        "### **10. Haz predicciones y evalúa el desempeño**\n",
        "\n",
        "¿Qué tan bien funciona el modelo?\n",
        "\n",
        "Evalúa la precisión de la clasificación del modelo con el set de prueba (test). También mira la matriz de confusión. En el caso de predecir solicitudes de tarjetas de crédito, es importante ver si el modelo es igualmente capaz de predecir el estado aprobado y denegado, de acuerdo con la frecuencia de estas etiquetas en el dataset original. Si el modelo no está funcionando bien en este aspecto, entonces podría terminar aprobando solicitudes que deberían haber sido denegadas. La matriz de confusión  ayuda a ver el desempeño del modelo en este aspecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZcA0QbCX4mm"
      },
      "outputs": [],
      "source": [
        "# Importa confusion_matrix\n",
        "# ... TU CÓDIGO AQUÍ ...\n",
        "\n",
        "# Usa logreg para predecir instancias del set de test y guárdalo\n",
        "y_pred = ...\n",
        "\n",
        "# Haz print de la precisión del modelo logreg\n",
        "print(\"Precisión del clasificador regresión logística: \", ...)\n",
        "\n",
        "# Haz print de la matriz de confusión del modelo logreg\n",
        "# ... TU CÓDIGO AQUÍ ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-IeD0S9ZUfN"
      },
      "source": [
        "### **11. Haz \"Grid searching\" y consigue un modelo que tenga mejor desempeño**\n",
        "\n",
        "Realiza una búsqueda de grilla de los hiperparámetros del modelo para mejorar su capacidad para predecir aprobaciones de tarjetas de crédito.\n",
        "\n",
        "La implementación de la regresión logística de scikit-learn consta de diferentes hiperparámetros, pero solo haz grid search sobre los dos siguientes:\n",
        "\n",
        "- tol: Usa los valores 0.01, 0.001, 0.0001 y 0.05\n",
        "- max_iter: Usa los valores 100, 150 y 200\n",
        "- solver: Usa los valores 'liblinear' y 'lbfgs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7vxUuOoYYw1"
      },
      "outputs": [],
      "source": [
        "# Importa GridSearchCV\n",
        "# ... TU CÓDIGO AQUÍ ...\n",
        "\n",
        "# Define los valores de grilla para tol y max_iter\n",
        "tol = ...\n",
        "max_iter = ...\n",
        "solver = ...\n",
        "\n",
        "# Crea un diccionario donde tol y max_iter son las llaves y las listas creadas previamente son sus valores correspondientes\n",
        "param_grid = dict(..., ..., ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd2cz8CIbPdi"
      },
      "source": [
        "### **12. Encuentra el modelo con mejor rendimiento**\n",
        "\n",
        "Haz definido la grilla de valores de hiperparámetros y los haz convertido en un formato de diccionario único que GridSearchCV() espera como uno de sus parámetros. Ahora, haz la búsqueda de grilla para ver qué valores funcionan mejor.\n",
        "\n",
        "Crea una instancia de GridSearchCV() con el modelo logreg anterior con todos los datos que tienes. También configura GridSearchCV() para que realice una validación cruzada de cinco pliegues (folds).\n",
        "\n",
        "Termina el examen almacenando la puntuación mejor lograda y los mejores hiperparámetros respectivos (si el accuracy no varía respecto al modelo entrenado previamente, indica el por qué en un comentario)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTdV7t0Xb-jf"
      },
      "outputs": [],
      "source": [
        "# Instancia GridSearchCV con los parámetros requeridos\n",
        "grid_model = GridSearchCV(estimator=..., param_grid=..., cv=5)\n",
        "\n",
        "# Entrena grid_model con los datos de entrenamiento\n",
        "grid_model_result = grid_model.fit(...)\n",
        "\n",
        "# Resume los resultados\n",
        "best_score, best_params = grid_model_result...., grid_model_result.....\n",
        "print(\"Mejor: %f usando %s\" % (..., ...))\n",
        "\n",
        "# Extrae el mejor modelo y evalúalo en el set de prueba\n",
        "best_model = ...\n",
        "print(\"Accuracy of logistic regression classifier: \", ...)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}